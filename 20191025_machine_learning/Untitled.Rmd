---
title: "20191025_class_8_machine_learning"
author: "Patrick Bruck"
date: "10/25/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


k-means example
```{r}
#Generate some example data for clustering
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))
  #cbind takes a vector and column-wise binds it
plot(x)
```


Use the kmeans() function setting k to 2 and nstart=20
Inspect/print the results
Q. How many points are in each cluster?
Q. What ‘component’ of your result object details
      - cluster size?
      - cluster assignment/membership?
      - cluster center?
Plot x colored by the kmeans cluster assignment and
      add cluster centers as blue points

```{r}
k <- kmeans(x, centers = 2, nstart = 20)
k
```


There are 9 outputs of kmeans function. You can view them by calling k$readout:
```{r}
k$size
k$cluster
plot(x, col = k$cluster)
  #for col function, numbers are diff colors. So this prints each point at a color that depends on   its cluster alignment
#Now we can also add our center points to the plot:
points(k$centers, col = "blue", pch=15)
```




Hierarchical Clustering
can be bottom-up or top-down. In bottom-up, it starts with each point as its own cluster. Then it groups the two closest and has one less cluster total, and so on.

Hierarchical clustering in R
```{r}
hc <- hclust(dist(x))
hc
  #printing hc is not very useful, like printing kmeans was
plot(hc)
  #this is more useful, gives a dendrogram. Note that it gives 2 main clusters all on its own,      without our direction to. This dendrogram comes directly from plotting distance between 2 points   like a soccer goal. The further the points, the higher the top of the goal has to be. These       clusters are those goals
  #you can draw a height line with abline(), and you can cut below a threshold with cutree()
abline(h=6, col="red")
abline(h=3.75, col="blue")
cutree(hc, h=3.75)
  #instead of trying to set a height, you can also just tell it to cut where k=a # of clusters
groups <- cutree(hc, k = 2)
plot(x, col=groups)
  #coloring by membership assigned by the cutting of the dendrogram into 2 groups
```




Let's do it with more realistic, overlapping data

```{r}
 # Step 1. Generate some example data for clustering
x <- rbind(
  matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2),   # c1
  matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
  matrix(c(rnorm(50, mean = 1, sd = 0.3),           # c3
           rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")
# Step 2. Plot the data without clustering
plot(x)
# Step 3. Generate colors for known clusters
#         (just so we can compare to hclust results)
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col)
```

Q. Use the dist(), hclust(), plot() and cutree() functions to return 2 and 3 clusters
Q. How does this compare to your known 'col' groups?
```{r}
hc2 <- hclust(dist(x))
plot(hc2)
groups2 <- cutree(hc2, k = 3)
plot(x, col=groups2)
```


Make a table of the groups:
```{r}
table(groups2)
  #note: we created 3 groups of 50, so many points are mis-clustered
table(col)
  #makes a table of the OG values
table(col, groups2)
  #makes a cross-ref table. Reads as: of c1, 35 are in cluster 1, 15 are in cluster 2, 0 in        cluster 3... 
```
